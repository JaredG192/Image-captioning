# ğŸ–¼ï¸ Image Captioning AI

This project is a deep learning-based image captioning model developed as part of a machine learning course. It automatically generates descriptive captions for input images using a combination of Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs).

---

## ğŸ“ Project Structure

- `caption.txt` â€“ Text file containing 5 captions for each image  
- `Image_Captioning.py` â€“ Python script containing the complete model pipeline: preprocessing, training, and caption generation  
- `Image_Captioning.ipynb` â€“ Jupyter Notebook version for exploratory experiments, visualizations, and model evaluation  
- `sample_images/` â€“ A small set of example images for testing and demonstration  

---

## ğŸ§ª Objectives

- Train a model that can generate natural language descriptions for unseen images  
- Preprocess image and text data for deep learning  
- Build a CNN-LSTM architecture to encode image features and decode them into captions  
- Evaluate model quality using BLEU scores and qualitative output  

---

## ğŸ› ï¸ Methods and Tools

**Languages & Libraries:**
- Python  
- TensorFlow / Keras  
- NumPy, Pandas  
- Matplotlib, Seaborn  
- NLTK, Pillow  

**Techniques:**
- Data Cleaning & Tokenization  
- Feature Extraction using Pretrained CNNs (e.g., InceptionV3)  
- Text Vectorization and Sequence Padding  
- RNN Decoder with LSTM  
- Evaluation using BLEU Score  

---

## ğŸ“Š Key Features and Components

- **Image Preprocessing** â€“ Images are resized and encoded using a pretrained CNN  
- **Text Preprocessing** â€“ Captions are cleaned, tokenized, and converted to sequences  
- **Vocabulary Creation** â€“ Tokenizer created for all unique words in the captions  
- **Model Architecture** â€“ Combines CNN encoder + LSTM decoder with attention mechanism (if included)  
- **Caption Generation** â€“ Greedy or beam search strategies used to predict the best caption  

---

## ğŸ“ˆ Results Summary

- The model was trained on the **Flickr8k** dataset  
- Achieved reasonable performance on test images with semantically correct and grammatically sound captions  
- BLEU scores were used to evaluate the quality of generated captions compared to human-written ones  

---

## ğŸ“‚ Dataset

The dataset consists of 8,000 images, each with 5 human-annotated captions.

ğŸ“¥ Due to GitHub file size limits, the dataset is not included in this repository.
ğŸ‘‰ [Flickr8k Image Captioning Dataset](https://www.kaggle.com/datasets/adityajn105/flickr8k)



---

## ğŸ‘¨â€ğŸ’» Authors

**Jared Gonzalez**, 
**A Sai Prasanth Reddy**, 
**Tejaswi Chigurupati**


California State University, San Bernardino  
Bachelor in Science, Computer Science,  
Minor in Data Science  

---

## ğŸ“ƒ License

This project is for academic purposes and is shared under the MIT License.




